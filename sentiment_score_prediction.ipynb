{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv('preprocessed_news_data.csv')\n",
    "\n",
    "# Ensure the 'processed_text' column exists\n",
    "if 'processed_text' not in new_df.columns:\n",
    "    raise ValueError(\"The input CSV must contain a 'processed_text' column.\")\n",
    "\n",
    "# Tokenize the processed text\n",
    "new_df['tokens'] = new_df['processed_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "w2v_model = Word2Vec.load(\"word2vec_model.model\")\n",
    "\n",
    "# Calculate IDF values using only the new dataset\n",
    "def calculate_idf(corpus):\n",
    "    total_documents = len(corpus)\n",
    "    word_document_count = Counter()\n",
    "    for document in corpus:\n",
    "        unique_words = set(document)\n",
    "        for word in unique_words:\n",
    "            word_document_count[word] += 1\n",
    "    idf_values = {word: math.log(total_documents / (1 + count)) for word, count in word_document_count.items()}\n",
    "    return idf_values\n",
    "\n",
    "# Use the new dataset to calculate IDF values\n",
    "new_corpus = new_df['tokens'].tolist()\n",
    "idf_values = calculate_idf(new_corpus)\n",
    "\n",
    "# Define function to calculate TF-IDF weighted document vectors\n",
    "def calculate_tfidf_weighted_doc_vector(tokens, idf_values, w2v_model):\n",
    "    tf_values = Counter(tokens)\n",
    "    total_terms = len(tokens)\n",
    "    tf_values = {word: count / total_terms for word, count in tf_values.items()}\n",
    "    weighted_sum = np.zeros(w2v_model.vector_size)\n",
    "    total_weight = 0.0\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv and token in idf_values:\n",
    "            tfidf_score = tf_values[token] * idf_values[token]\n",
    "            weighted_sum += w2v_model.wv[token] * tfidf_score\n",
    "            total_weight += tfidf_score\n",
    "    if total_weight > 0:\n",
    "        return weighted_sum / total_weight\n",
    "    else:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# Generate document vectors for the new dataset\n",
    "new_doc_vectors = []\n",
    "for tokens in new_df['tokens']:\n",
    "    doc_vector = calculate_tfidf_weighted_doc_vector(tokens, idf_values, w2v_model)\n",
    "    new_doc_vectors.append(doc_vector)\n",
    "\n",
    "new_doc_vectors = np.array(new_doc_vectors)\n",
    "\n",
    "# Load the trained XGBoost model from pickle file\n",
    "with open('xgboost_model.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "# Convert document vectors to DMatrix format for prediction\n",
    "dnew = xgb.DMatrix(new_doc_vectors)\n",
    "\n",
    "# Predict sentiment scores using the trained XGBoost model\n",
    "new_df['sentiment_score'] = xgb_model.predict(dnew)\n",
    "\n",
    "# Save the updated DataFrame with sentiment scores to a new CSV file\n",
    "new_df.to_csv('new_articles_with_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Sentiment scores have been added and saved to 'new_articles_with_sentiment.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
